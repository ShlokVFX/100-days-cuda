Mentor: https://github.com/hkproj/

Discord : https://discord.gg/4Tg4TkJQzE

Instructions: https://github.com/hkproj/100-days-of-cuda

# 100 Days of CUDA Learning

This repository documents my 100-day journey of learning CUDA programming, writing optimized kernels, and improving GPU performance.

| Day  | Output Summary | Notes | Link |
|------|--------------|-------|------|
| 1    |  [Vector Addition Kernel](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2001/Output.png)  | Learned basic CUDA syntax and kernel execution - Vector Addtion and printing Hello Cuda. | [Day 1](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2001/Readme.md) |
| 2    | [Benchmarking Vector Add](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2002/BenchmarkVectorAdd.png) | Explored about Benchmarking in Cuda with Vector Add. | [Day 2](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2002/Readme.md) |
| 3    |  [Cuda Streams](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2003/CudaStreams_result.png) [AtomicAddtion](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2003/AtomicAdditionResult.png) |CUDA Stream is a sequence of operations (memory transfers, kernel launches, etc.) that execute in order within the stream, but operations in different streams can run concurrently. | [Day 3](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2003/Readme.md) |
| 4    | [Unified Mem VectorAdd](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2004/VectorAdd_withErrorCheck.png)  |  Unified Memory simplifies memory management by allowing the CPU and GPU to share the same memory space. | [Day 4](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2004/Readme.md) |
| 5    | [Tiled Matrix Multiplication](https://github.com/ShlokVFX/100-days-cuda/blob/main/Day%2004/VectorAdd_withErrorCheck.png)  | Implemented Tiled Matrix Multiplication in CUDA using shared memory to optimize performance | [Day 5](./day5/) |


## Goals:
- Understand CUDA fundamentals.
- Write optimized and efficient GPU kernels.
- Explore memory hierarchy, warp scheduling, and Tensor Cores.
- Apply CUDA to deep learning and high-performance computing.

