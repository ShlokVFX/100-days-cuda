# ğŸš€ 100 Days of CUDA - Learning Plan

## ğŸ† Goal
Master **CUDA, cuBLAS, cuDNN, Parallel Computing, Performance Optimization, and C++** by building optimized GPU programs over 100 days.

## ğŸ“… Roadmap

### **Week 1-2: Foundations of CUDA & Parallel Computing**
ğŸ”¹ **Goals:**
- Learn CUDA programming model, memory hierarchy, and basic kernel execution.

ğŸ”¹ **Topics:**
âœ… CUDA installation & setup  
âœ… Basics of GPU architecture & memory types (Global, Shared, Local, Registers)  
âœ… Writing first CUDA kernel (`__global__` functions)  
âœ… Thread hierarchy: Blocks, Grids, Warps  
âœ… Understanding execution model: SIMD, SIMT  

ğŸ”¹ **Hands-on:**
- Implement **vector addition** using CUDA.
- Experiment with different **block and grid sizes**.
- Measure performance using **CUDA events**.

---
### **Week 3-4: Memory Management & Performance Optimization (Level 1)**
ğŸ”¹ **Goals:**
- Learn CUDA memory hierarchy & optimization techniques.

ğŸ”¹ **Topics:**
âœ… Host vs Device Memory, Unified Memory  
âœ… Global memory coalescing & bank conflicts  
âœ… Shared memory & its impact on performance  
âœ… Memory bandwidth considerations  

ğŸ”¹ **Hands-on:**
- Implement **Matrix Multiplication using Global Memory**.
- Optimize it using **Shared Memory**.
- Compare performance (Global vs Shared).

---
### **Week 5-6: Advanced CUDA - Streams, Asynchronous Execution, Profiling**
ğŸ”¹ **Goals:**
- Learn CUDA Streams & parallel execution techniques.

ğŸ”¹ **Topics:**
âœ… CUDA Streams & concurrent kernel execution  
âœ… CUDA events for timing & profiling  
âœ… Asynchronous memory transfers (`cudaMemcpyAsync`)  
âœ… Profiling with **NVIDIA Nsight, nvprof**  

ğŸ”¹ **Hands-on:**
- Implement **pipelined memory transfers** with `cudaMemcpyAsync`.
- Launch **multiple kernels in parallel** using Streams.

---
### **Week 7-8: cuBLAS - Accelerating Linear Algebra on GPU**
ğŸ”¹ **Goals:**
- Learn how to use cuBLAS for BLAS (Basic Linear Algebra Subroutines).

ğŸ”¹ **Topics:**
âœ… Introduction to cuBLAS API  
âœ… cuBLAS matrix-vector, matrix-matrix operations  
âœ… Strided and batched matrix multiplications  
âœ… Memory management in cuBLAS  

ğŸ”¹ **Hands-on:**
- Implement **Matrix Multiplication using cuBLAS**.
- Compare cuBLAS vs naive CUDA implementation.
- Implement **Batch GEMM (General Matrix Multiplication)**.

---
### **Week 9-10: cuDNN - Accelerating Deep Learning on GPU**
ğŸ”¹ **Goals:**
- Learn cuDNN for optimized deep learning operations.

ğŸ”¹ **Topics:**
âœ… Convolutions in cuDNN  
âœ… cuDNN tensor descriptors & data layout  
âœ… Pooling, Activation, Softmax  
âœ… Performance tuning in cuDNN  

ğŸ”¹ **Hands-on:**
- Implement **CNN Forward Pass using cuDNN**.
- Benchmark cuDNN convolutions against a naive implementation.

---
### **Week 11-12: Performance Optimization (Level 2)**
ğŸ”¹ **Goals:**
- Advanced CUDA optimizations for performance tuning.

ğŸ”¹ **Topics:**
âœ… Register usage optimization  
âœ… Occupancy calculator & warp-level programming  
âœ… CUDA Cooperative Groups  
âœ… Tensor Cores & mixed precision computing  

ğŸ”¹ **Hands-on:**
- Optimize **matrix multiplication using Tensor Cores**.
- Implement **warp-shuffle based reduction**.

---
### **Week 13-14: Multi-GPU Programming**
ğŸ”¹ **Goals:**
- Learn multi-GPU communication and workload distribution.

ğŸ”¹ **Topics:**
âœ… Multi-GPU architecture & peer-to-peer communication  
âœ… `cudaMemcpyPeer()` and NVLink  
âœ… Using NCCL for distributed computing  

ğŸ”¹ **Hands-on:**
- Implement **data parallelism using multi-GPU**.
- Optimize **inter-GPU communication**.

---
### **Week 15: Putting It All Together - Final Optimization Challenges**
ğŸ”¹ **Goals:**
- Apply all learned concepts to real-world applications.

ğŸ”¹ **Projects & Challenges:**
âœ… Optimize **large-scale matrix multiplication (multi-GPU + cuBLAS)**  
âœ… Implement **efficient CNN training pipeline using cuDNN**  
âœ… Profile & optimize an **end-to-end CUDA application**  

---
### **ğŸ¯ Final Milestone (Day 100)**
ğŸš€ **Publish a summary report + code repo** showcasing:
- Performance benchmarks
- Optimized CUDA applications
- Lessons learned + best practices  

---
## **ğŸ”¥ Bonus (If Time Permits)**
âœ… Deep dive into **CUTLASS for custom GEMM optimizations**  
âœ… Learn **CUDA Graphs for efficient kernel execution**  
âœ… Experiment with **NVIDIA Triton for inference acceleration**  

---
## **ğŸ’¡ Summary**
- **Week 1-2** â†’ CUDA Basics + Parallel Computing  
- **Week 3-4** â†’ Memory Optimization  
- **Week 5-6** â†’ Streams + Profiling  
- **Week 7-8** â†’ cuBLAS  
- **Week 9-10** â†’ cuDNN  
- **Week 11-12** â†’ Advanced Optimization  
- **Week 13-14** â†’ Multi-GPU Programming  
- **Week 15** â†’ Final Projects & Optimization  

## ğŸ† Goal
Master **CUDA, cuBLAS, cuDNN, Parallel Computing, Performance Optimization, and C++** by building optimized GPU programs over 100 days.
